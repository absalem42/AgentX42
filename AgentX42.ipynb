{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105039,"databundleVersionId":12838406,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b8be7787-ff46-4a01-913c-e157e2823f57","_cell_guid":"8d23f6f9-616a-48c8-a02b-35614c9b210c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Cell 1 : paths  +  quick sanity\n# ===============================\nimport os, glob, json, random\nfrom pathlib import Path\nimport pandas as pd\nfrom collections import Counter\nimport torch\n\n# -------- 1. Base paths --------\nBASE = Path('/kaggle/input/identity-employees-in-surveillance-cctv')\n\n# detect whether train lives under â€œdataset/trainâ€ or â€œdataset/dataset/trainâ€\npossible_roots = [BASE / 'dataset', BASE / 'dataset' / 'dataset']\nfor candidate in possible_roots:\n    if (candidate / 'train' / 'images').exists():\n        ROOT = candidate\n        break\nelse:\n    raise FileNotFoundError(f\"Could not locate train/images under {possible_roots}\")\n\n# define train / unseen-test paths\nTRAIN_IMG_DIR = ROOT / 'train' / 'images'\nLABELS_CSV    = ROOT / 'train' / 'labels.csv'\nUNSEEN_ROOT   = BASE / 'dataset_unseen'\nTEST_IMG_DIR  = UNSEEN_ROOT / 'unseen_test' / 'images'\n\n# -------- 2. Sanity checks --------\nfor p in (TRAIN_IMG_DIR, TEST_IMG_DIR, LABELS_CSV):\n    assert p.exists(), f\"Missing: {p}\"\n\nprint(\"Train images :\", len(list(TRAIN_IMG_DIR.glob('*.jpg'))))\nprint(\" Test images :\", len(list(TEST_IMG_DIR.glob('*.jpg'))))\n\n# -------- 3. Read & CLEAN labels_df --------\nlabels_df = pd.read_csv(LABELS_CSV)\n\n# âœ¨ NEW: drop rows pointing to images that are gone (e.g. face_1147.jpg)\nmask_exists = labels_df['filename'].apply(lambda f: (TRAIN_IMG_DIR / f).exists())\nlabels_df   = labels_df[mask_exists].reset_index(drop=True)\n\nprint(f\"After dropping missing images â†’ {len(labels_df):,} rows remain\")\ndisplay(labels_df.head())\n\n# -------- 4. Quick counts (optional) --------\nemp_counts = Counter(labels_df['emp_id'])\nprint(f\"Unique employees: {len(emp_counts)}\")\nprint(\"Five smallest classes:\", emp_counts.most_common()[-5:])\n\n# -------- 5. GPU check (optional) --------\nprint(\"CUDA available :\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU name       :\", torch.cuda.get_device_name(0))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell A : extract 500 JPG frames / employee from reference video\n#          + save them in EXTRA_DIR  +  create new_rows list\n# ================================================================\nimport cv2, numpy as np\nfrom pathlib import Path\n\nEXTRA_DIR = Path('/kaggle/working/tmp_ref_frames')\nEXTRA_DIR.mkdir(parents=True, exist_ok=True)\n\n\nnew_rows = []                 # â† will be concatenated with labels_df in Cell 2\nSAMPLES  = 500                # number of evenly-spaced frames per video\n\nfor emp_dir in (ROOT / 'reference_faces').iterdir():        # emp001, emp002 â€¦\n    vid_files = list(emp_dir.glob('*.mp4'))\n    if not vid_files:\n        continue                               # some employees may have only JPGs\n    vid = vid_files[0]\n    cap = cv2.VideoCapture(str(vid))\n    if not cap.isOpened():\n        print(f\"âš ï¸ could not open {vid}\")\n        continue\n\n    tot = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # ----- sample SAMPLES indices evenly across the video ----------\n    for f in np.linspace(0, tot - 1, SAMPLES, dtype=int):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n        ok, frame = cap.read()\n        if not ok:\n            continue\n        fname = f'{emp_dir.name}_{f:04d}.jpg'               # emp001_0003.jpg â€¦\n        cv2.imwrite(str(EXTRA_DIR / fname), frame)\n        new_rows.append({'filename': fname, 'emp_id': emp_dir.name})\n\n    cap.release()\n\nprint(f\"âœ… Saved {len(new_rows):,} extra frames to {EXTRA_DIR}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell 2 : dataset, transforms, stratified split (CCTV hold-out)\n# ================================================================\nimport subprocess, sys, importlib.util, numpy as np, torch, random\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom collections import Counter\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\nfrom io import BytesIO\n\n# 0ï¸âƒ£  Install timm (once per kernel) --------------------------------\nif importlib.util.find_spec(\"timm\") is None:\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n                    \"timm\", \"torchmetrics\", \"--no-progress\"])\n\n# -------------------------------------------------------------------\n# 1ï¸âƒ£  Build master labels_df\n#     (Cell 1 already loaded + cleaned labels_df, so we start there)\n# -------------------------------------------------------------------\nlabels_df = labels_df.copy()                 # make local copy of clean df\n\n# 1.0  Append the 500-frame video samples made in Cell A\nextra_df  = pd.DataFrame(new_rows)           # list produced in Cell A\nlabels_df = pd.concat([labels_df, extra_df], ignore_index=True)\n\n# 1.1  Append static reference JPGs as â€œphotoâ€ rows\nstatic_rows = []\nfor emp_dir in (ROOT / 'reference_faces').iterdir():\n    for jpg in emp_dir.glob('*.jpg'):\n        static_rows.append({'filename': jpg.name,\n                            'emp_id':   emp_dir.name})\n        # symlink each static JPG into EXTRA_DIR so IMG_ROOTS can find it\n        link = EXTRA_DIR / jpg.name\n        if not link.exists():\n            link.symlink_to(jpg)\nlabels_df = pd.concat([labels_df, pd.DataFrame(static_rows)],\n                      ignore_index=True)\n\n# 1.2  Final safety filter â€” keep only rows whose file exists\nIMG_ROOTS = [TRAIN_IMG_DIR, EXTRA_DIR]       # search order for images\nlabels_df = labels_df[\n    labels_df['filename'].apply(\n        lambda f: any((root / f).exists() for root in IMG_ROOTS)\n    )\n].reset_index(drop=True)\nprint(f\"After drop-missing âœ {len(labels_df):,} rows\")\n\n# -------------------------------------------------------------------\n# 2ï¸âƒ£  Encode employee IDs â†’ int labels\n# -------------------------------------------------------------------\nemployee_ids = sorted(labels_df.emp_id.unique())\nemp2idx = {e: i for i, e in enumerate(employee_ids)}\nlabels_df[\"label_idx\"] = labels_df.emp_id.map(emp2idx)\n\n# 3ï¸âƒ£  Tag each row as CCTV vs photo\nlabels_df[\"source\"] = labels_df.filename.apply(\n    lambda fn: \"photo\" if (EXTRA_DIR / fn).exists() else \"cctv\"\n)\n\n# -------------------------------------------------------------------\n# 4ï¸âƒ£  Transforms\n# -------------------------------------------------------------------\nclass JpegCompression:                       # PIL-only artefact\n    def __init__(self, quality=(30, 60)):\n        self.qmin, self.qmax = quality\n    def __call__(self, img: Image.Image):\n        buf = BytesIO()\n        q   = random.randint(self.qmin, self.qmax)\n        img.save(buf, format=\"JPEG\", quality=q)\n        buf.seek(0)\n        return Image.open(buf)\n\ntrain_tfms = T.Compose([\n    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    T.RandomHorizontalFlip(0.5),\n    T.ColorJitter(0.2, 0.2, 0.2, 0.1),\n    T.RandomRotation(15),\n    T.RandomPerspective(0.3, p=0.2),\n    T.RandomApply([T.GaussianBlur(3)], p=0.2),\n    T.RandomApply([JpegCompression((30, 60))], p=0.3),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406],\n                [0.229, 0.224, 0.225]),\n    T.RandomErasing(p=0.3, scale=(0.02, 0.10), ratio=(0.3, 3.3)),\n])\n\nval_tfms = T.Compose([\n    T.Resize(256), T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize([0.485, 0.456, 0.406],\n                [0.229, 0.224, 0.225]),\n])\n\n# -------------------------------------------------------------------\n# 5ï¸âƒ£  Dataset (multi-root lookup)\n# -------------------------------------------------------------------\nclass FaceDS(Dataset):\n    def __init__(self, df, tfm):\n        self.df   = df.reset_index(drop=True)\n        self.tfm  = tfm\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        for root in IMG_ROOTS:\n            p = root / row.filename\n            if p.exists():\n                img = Image.open(p).convert(\"RGB\")\n                break\n        else:\n            raise FileNotFoundError(f\"Missing {row.filename}\")\n        return self.tfm(img), row.label_idx\n\nfull_ds = FaceDS(labels_df, train_tfms)\n\n# -------------------------------------------------------------------\n# 6ï¸âƒ£  Stratified 10 % hold-out on CCTV (keep all photos in train)\n# -------------------------------------------------------------------\ncctv_idx  = labels_df.index[labels_df.source == \"cctv\"].to_numpy()\ncctv_lbls = labels_df.label_idx.loc[cctv_idx].to_numpy()\n\n# keep singleton classes entirely in train\nfrom collections import Counter\ncnt = Counter(cctv_lbls)\nsingleton_cls = [cls for cls, n in cnt.items() if n == 1]\nmask_single   = np.isin(cctv_lbls, singleton_cls)\nsingle_idx    = cctv_idx[mask_single]\nmulti_idx     = cctv_idx[~mask_single]\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=42)\ntrain_multi, val_multi = next(sss.split(multi_idx,\n                                        labels_df.label_idx.loc[multi_idx]))\ntrain_cctv = np.concatenate([single_idx, multi_idx[train_multi]])\nval_cctv   = multi_idx[val_multi]\n\nphoto_idx  = labels_df.index[labels_df.source == \"photo\"].to_numpy()\n\ntrain_idx = np.concatenate([train_cctv, photo_idx])\nval_idx   = val_cctv\n\n# -------------------------------------------------------------------\n# 7ï¸âƒ£  DataLoaders\n# -------------------------------------------------------------------\ntrain_ds = Subset(full_ds, train_idx)\nval_ds   = Subset(FaceDS(labels_df.loc[val_idx], val_tfms),\n                  np.arange(len(val_idx)))\n\nBATCH = 32\ntrain_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n                      num_workers=4, pin_memory=True)\nval_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,\n                      num_workers=4, pin_memory=True)\n\nprint(f\"Train images: {len(train_ds):,} | Val images: {len(val_ds):,}\")\nprint(\"Smallest training classes:\",\n      Counter(labels_df.label_idx.loc[train_idx]).most_common()[-5:])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell 3 : EfficientNet-B0 + ArcFace fine-tune (100 epochs)\n# ================================================================\nimport timm, torch, math, numpy as np\nfrom torch import nn\nfrom torchmetrics.classification import MulticlassAccuracy\n\ndevice       = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_CLASSES  = len(employee_ids)                 # set in Cell 2\nEPOCHS       = 70\nCKPT         = '/kaggle/working/effb0_arcface.pt'\n\n# â”€â”€ 1ï¸âƒ£  Backbone + 512-D projection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nbackbone = timm.create_model(\n    'efficientnet_b0', pretrained=True, num_classes=0, global_pool='avg'\n).to(device)\nproj = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n\n# â”€â”€ 2ï¸âƒ£  ArcFace additive-margin head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass ArcFace(nn.Module):\n    def __init__(self, in_f, out_f, s=30.0, m=0.50):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(out_f, in_f))\n        nn.init.xavier_uniform_(self.W)\n        self.s, self.m = s, m\n        self.cos_m, self.sin_m = math.cos(m), math.sin(m)\n        self.th, self.mm = math.cos(math.pi - m), math.sin(math.pi - m) * m\n    def forward(self, x, labels):\n        x_n = nn.functional.normalize(x)\n        w_n = nn.functional.normalize(self.W)\n        cos = nn.functional.linear(x_n, w_n)            # cosine Î¸\n        sin = torch.sqrt(1.0 - torch.clamp(cos**2, 0, 1))\n        phi = cos * self.cos_m - sin * self.sin_m        # cos(Î¸+m)\n        phi = torch.where(cos > self.th, phi, cos - self.mm)\n        one_hot = torch.zeros_like(cos, device=x.device)\n        one_hot.scatter_(1, labels.view(-1,1), 1.0)\n        logits = (one_hot * phi + (1.0 - one_hot) * cos) * self.s\n        return logits\n\narc = ArcFace(512, NUM_CLASSES).to(device)\n\n# â”€â”€ 3ï¸âƒ£  Optimizer, warm-up, cosine LR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nparams = list(backbone.parameters()) + list(proj.parameters()) + list(arc.parameters())\nopt    = torch.optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nwarm   = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=2)\nsched  = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\nmetric    = MulticlassAccuracy(num_classes=NUM_CLASSES, average='macro').to(device)\nbest_acc  = 0.0\n\n# â”€â”€ 4ï¸âƒ£  Training loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfor ep in range(1, EPOCHS + 1):\n    # ---- train ----\n    backbone.train(); proj.train(); arc.train()\n    for x, y in train_dl:                       # train_dl from Cell 2\n        x, y = x.to(device), y.to(device)\n        feats  = proj(backbone(x))\n        logits = arc(feats, y)\n        loss   = criterion(logits, y)\n\n        opt.zero_grad(); loss.backward(); opt.step()\n\n    # ---- scheduler ----\n    (warm if ep <= 2 else sched).step()\n\n    # ---- validation ----\n    backbone.eval(); proj.eval(); metric.reset()\n    with torch.no_grad():\n        # pre-normalize weight matrix once per epoch\n        Wn = nn.functional.normalize(arc.W, dim=1)      # [C,512]\n        for x, y in val_dl:                             # val_dl from Cell 2\n            x, y = x.to(device), y.to(device)\n            feats = proj(backbone(x))\n            fnorm = nn.functional.normalize(feats, dim=1)\n            preds = torch.matmul(fnorm, Wn.T).argmax(1) # cosine scores\n            metric.update(preds, y)\n\n    acc = metric.compute().item()\n    print(f\"Epoch {ep:03d} | Val Macro-Acc {acc:.4f}\")\n\n    if acc > best_acc:\n        best_acc = acc\n        torch.save({'backbone': backbone.state_dict(),\n                    'proj'    : proj.state_dict(),\n                    'arc'     : arc.state_dict()}, CKPT)\n        print(\"  ğŸ”¥ new best saved\")\n\nprint(f\"\\nâ–¶ Best validation Macro-Accuracy: {best_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --------------------------------------------------\n# after reading labels.csv  (Cell 1 or wherever)\n# --------------------------------------------------\nTRAIN_IMG_DIR = Path('/kaggle/input/identity-employees-in-surveillance-cctv'\n                     '/dataset/dataset/train/images')\n\n# keep only rows whose image file is really on disk\nlabels_df = labels_df[\n    labels_df['filename'].apply(lambda f: (TRAIN_IMG_DIR / f).exists())\n].reset_index(drop=True)\n\nprint(f\"Filtered labels_df â†’ {len(labels_df):,} rows remain\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell 4 : build FIQA-filtered gallery + sweep cosine Ï„\n# ================================================================\nimport sys, os, gc\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport timm\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms as T\nfrom insightface.app import FaceAnalysis\n\n# â”€â”€ device & base paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nBASE   = Path('/kaggle/input/identity-employees-in-surveillance-cctv')\n\n# â”€â”€ updated dataset paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nTRAIN_ROOT   = BASE / 'dataset' / 'dataset'\nREF_DIR      = TRAIN_ROOT / 'reference_faces'\nEXTRA_DIR = Path('/kaggle/working/tmp_ref_frames')  # updated to where frames were saved\n\n# â”€â”€ sanity-check reference_faces â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif not REF_DIR.exists():\n    raise FileNotFoundError(f\"Missing required directory: {REF_DIR}\")\n\n# â”€â”€ check for extracted frames; skip if none â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nif EXTRA_DIR.exists() and any(EXTRA_DIR.glob('*.jpg')):\n    extra_imgs = list(EXTRA_DIR.glob('*.jpg'))\n    print(f\"Found {len(extra_imgs)} extracted frames in {EXTRA_DIR}\")\n    use_extra = True\nelse:\n    print(f\"âš ï¸  No extracted frames in {EXTRA_DIR}, skipping video-frames step.\")\n    extra_imgs = []\n    use_extra = False\n\n# â”€â”€ reload trained backbone + projection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nCKPT     = '/kaggle/working/effb0_arcface.pt'\nbackbone = timm.create_model('efficientnet_b0', pretrained=False,\n                             num_classes=0, global_pool='avg').to(device)\nproj     = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n\nck = torch.load(CKPT, map_location=device)\nbackbone.load_state_dict(ck['backbone'])\nproj.load_state_dict(ck['proj'])\nbackbone.eval(); proj.eval()\n\n# â”€â”€ FIQA quality model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nqa = FaceAnalysis(name='buffalo_s')\nqa.prepare(ctx_id=0)\n\n# â”€â”€ transforms for gallery & val â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nval_tfms = T.Compose([\n    T.Resize(256), T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\n# â”€â”€ build raw gallery (static JPEGs + optional video frames) â”€â”€â”€â”€\nref_paths, ref_labels, ref_embs = [], [], []\n\n# 1) high-quality reference photos\nfor emp_dir in REF_DIR.iterdir():\n    for jpg in emp_dir.glob('*.jpg'):\n        img = Image.open(jpg).convert('RGB')\n        ref_paths.append(jpg)\n        ref_labels.append(emp_dir.name)\n        with torch.no_grad():\n            emb = proj(backbone(val_tfms(img).unsqueeze(0).to(device))).squeeze().cpu()\n        ref_embs.append(emb)\n\n# 2) optionally include extracted video frames\nif use_extra:\n    for jpg in extra_imgs:\n        img = Image.open(jpg).convert('RGB')\n        ref_paths.append(jpg)\n        ref_labels.append(jpg.stem.split('_')[0])\n        with torch.no_grad():\n            emb = proj(backbone(val_tfms(img).unsqueeze(0).to(device))).squeeze().cpu()\n        ref_embs.append(emb)\n\nprint(\"Raw gallery size :\", len(ref_embs))\n\n# â”€â”€ FIQA filtering: drop bottom 30% by det_score â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nscores = []\nfor p in tqdm(ref_paths, desc=\"FIQA scoring\"):\n    arr   = np.array(Image.open(p).convert('RGB'))\n    faces = qa.get(arr)\n    scores.append(faces[0].det_score if faces else 0.0)\n\nthreshold = np.percentile(scores, 20)\nkeep_ix   = [i for i, s in enumerate(scores) if s >= threshold]\n\nref_embs   = torch.stack([ref_embs[i] for i in keep_ix])\nref_labels = [ref_labels[i]     for i in keep_ix]\nref_norm   = nn.functional.normalize(ref_embs, dim=1)\nprint(\"Filtered gallery:\", ref_norm.shape)\n\n# â”€â”€ embed validation CCTV set & sweep Ï„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nval_embs, val_true = [], []\nidx2emp = {v: k for k, v in emp2idx.items()}\n\nfor xb, yb in tqdm(val_dl, desc=\"val embeddings\"):\n    xb = xb.to(device)\n    with torch.no_grad():\n        feats = backbone(xb)\n        embs  = proj(feats).cpu()\n    val_embs.append(embs)\n    val_true.extend([idx2emp[int(i)] for i in yb])\n\nval_embs = torch.cat(val_embs, dim=0)\n\ndef macro_acc(true, pred):\n    classes = sorted(set(true))\n    return np.mean([\n        np.mean([t == p for t, p in zip(true, pred) if t == c])\n        for c in classes\n    ])\n\nbest_tau, best_ma = None, -1\nfor Ï„ in np.linspace(0.30, 0.55, 51):\n    preds = []\n    for e in val_embs:\n        sims = ref_norm @ nn.functional.normalize(e, dim=0)\n        j    = sims.argmax().item()\n        preds.append(ref_labels[j] if sims[j] >= Ï„ else \"unknown\")\n    m = macro_acc(val_true, preds)\n    print(f\"Ï„={Ï„:.2f} â†’ Val Macro-Acc {m:.4f}\")\n    if m > best_ma:\n        best_ma, best_tau = m, Ï„\n\nprint(f\"\\nâ–¶ Best Ï„ = {best_tau:.2f}  (Val Macro-Acc = {best_ma:.4f})\")\n\n# â”€â”€ save gallery + threshold for Cell 5 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntorch.save({\n    'ref_norm':   ref_norm,\n    'ref_labels': ref_labels,\n    'best_tau':   best_tau\n}, '/kaggle/working/gallery.pt')\n\n# cleanup\ngc.collect()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell 4B : grid-search (P_TH, Ï„) on validation set\n# ================================================================\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport timm\nfrom itertools import product\n\ndevice      = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_CLASSES = len(employee_ids)\n\n# â”€â”€ 1ï¸âƒ£ Redefine ArcFace to match your checkpoint keys â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nclass ArcFace(nn.Module):\n    def __init__(self, in_f, out_f, s=30.0, m=0.50):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(out_f, in_f))\n        nn.init.xavier_uniform_(self.W)\n        self.s, self.m = s, m\n        self.cos_m, self.sin_m = np.cos(m), np.sin(m)\n        self.th, self.mm = np.cos(np.pi - m), np.sin(np.pi - m) * m\n\n    def forward(self, x, y):\n        x_n = nn.functional.normalize(x)\n        w_n = nn.functional.normalize(self.W)\n        cos = nn.functional.linear(x_n, w_n)\n        sin = torch.sqrt(1.0 - torch.clamp(cos**2, 0, 1))\n        phi = torch.where(cos > self.th,\n                          cos * self.cos_m - sin * self.sin_m,\n                          cos - self.mm)\n        one_hot = torch.zeros_like(cos, device=x.device)\n        one_hot.scatter_(1, y.view(-1,1), 1.0)\n        logits = (one_hot * phi + (1.0 - one_hot) * cos) * self.s\n        return logits\n\n# â”€â”€ 2ï¸âƒ£ Reload your classifier head to get soft-max predictions â”€â”€â”€\nck = torch.load('/kaggle/working/effb0_arcface.pt', map_location=device)\n\nbackbone = timm.create_model('efficientnet_b0', pretrained=False,\n                             num_classes=0, global_pool='avg').to(device)\nproj     = nn.Linear(backbone.num_features, 512, bias=False).to(device)\narc      = ArcFace(512, NUM_CLASSES).to(device)\n\nbackbone.load_state_dict(ck['backbone'])\nproj.load_state_dict(ck['proj'])\narc.load_state_dict( ck['arc'] )\n\nbackbone.eval(); proj.eval(); arc.eval()\n\n@torch.no_grad()\ndef softmax_vec(x):\n    feats  = proj(backbone(x))\n    dummy  = torch.zeros(len(x), dtype=torch.long, device=device)\n    logits = arc(feats, dummy)\n    return torch.softmax(logits, dim=1)\n\n# â”€â”€ 3ï¸âƒ£ Precompute soft-max on your val set â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npvecs = []\nfor imgs, _ in val_dl:\n    pvecs.append( softmax_vec(imgs.to(device)).cpu() )\npvecs_val = torch.cat(pvecs)  # [N_val, NUM_CLASSES]\n\n# â”€â”€ 4ï¸âƒ£ Macro-accuracy helper â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef macro_acc(true, pred):\n    classes = sorted(set(true))\n    return np.mean([\n        np.mean([t==p for t,p in zip(true,pred) if t==c])\n        for c in classes\n    ])\n\n# (we assume the following variables are in scope from Cell 4):\n#   ref_norm    # torch.Tensor [N_ref, 512]\n#   ref_labels  # list[str] of length N_ref\n#   best_tau    # float from Cell 4\n#   val_embs    # torch.Tensor [N_val, 512]\n#   val_true    # list[str] of length N_val\n\n# â”€â”€ 5ï¸âƒ£ Grid-search over P_TH âˆˆ [0.50,0.70] and Ï„ Â±0.03 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nP_grid = np.arange(0.50, 0.71, 0.02)\nT_grid = np.arange(best_tau-0.03, best_tau+0.031, 0.01)\n\nbest_pair, best_ma = None, -1\nprint(f\"Searching PÃ—Ï„ grid with {len(P_grid)}Ã—{len(T_grid)} combosâ€¦\")\n\nfor P_TH, Ï„ in product(P_grid, T_grid):\n    preds = []\n    for prob_vec, emb in zip(pvecs_val, val_embs):\n        top_p, top_i = prob_vec.max(0)\n        if top_p >= P_TH:\n            preds.append(employee_ids[int(top_i)])\n        else:\n            sims = ref_norm @ nn.functional.normalize(emb, dim=0)\n            j    = int(sims.argmax())\n            preds.append(ref_labels[j] if sims[j] >= Ï„ else \"unknown\")\n    m = macro_acc(val_true, preds)\n    if m > best_ma:\n        best_ma, best_pair = m, (P_TH, Ï„)\n    print(f\"P={P_TH:.2f}, Ï„={Ï„:.2f} â†’ Val Macro-Acc {m:.4f}\")\n\nprint(f\"\\nâ–¶ BEST thresholds: P_TH = {best_pair[0]:.2f}, Ï„ = {best_pair[1]:.2f}   (Val Macro-Acc = {best_ma:.4f})\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell 5 : hybrid soft-max + gallery â†’ submission.csv\n# ================================================================\nimport torch\nimport pandas as pd\nimport timm\nfrom PIL import Image\n\ndevice      = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_CLASSES = len(employee_ids)\nCKPT        = '/kaggle/working/effb0_arcface.pt'\n\n# â”€â”€ 1ï¸âƒ£ Plug in your best thresholds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nP_TH = best_pair[0]   # from Cell 4B\nC_TH = best_pair[1]   # from Cell 4B\n\n# â”€â”€ 2ï¸âƒ£ Reload backbone + ArcFace head â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# (ArcFace class must already be defined in your notebook)\nbackbone = timm.create_model('efficientnet_b0', pretrained=False,\n                             num_classes=0, global_pool='avg').to(device)\nproj     = torch.nn.Linear(backbone.num_features, 512, bias=False).to(device)\narc      = ArcFace(512, NUM_CLASSES).to(device)\n\nck = torch.load(CKPT, map_location=device)\nbackbone.load_state_dict( ck['backbone'] )\nproj.load_state_dict(     ck['proj']     )\narc.load_state_dict(      ck['arc']      )\n\nbackbone.eval(); proj.eval(); arc.eval()\n\n# â”€â”€ 3ï¸âƒ£ Define helper functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n@torch.no_grad()\ndef softmax_vec(x):\n    feats  = proj(backbone(x))\n    dummy  = torch.zeros(len(x), dtype=torch.long, device=device)\n    logits = arc(feats, dummy)\n    return torch.softmax(logits, dim=1)\n\n@torch.no_grad()\ndef embed(timg):\n    t = val_tfms(timg).unsqueeze(0).to(device)\n    return proj(backbone(t)).squeeze().cpu()\n\n# â”€â”€ 4ï¸âƒ£ Inference loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nrows = []\nfor img_path in sorted(TEST_IMG_DIR.glob('*.jpg')):\n    pil = Image.open(img_path).convert('RGB')\n    # classifier branch with softmax\n    x      = val_tfms(pil).unsqueeze(0).to(device)\n    pvec   = softmax_vec(x)[0].cpu()\n    top_p, top_i = pvec.max(0)\n\n    # gallery branch with cosine similarity\n    emb    = embed(pil)\n    emb_n  = torch.nn.functional.normalize(emb, dim=0)\n    sims   = ref_norm @ emb_n         # ref_norm & ref_labels from Cell 4\n    j      = int(sims.argmax())\n\n    # hybrid decision\n    if top_p >= P_TH:\n        pred = employee_ids[int(top_i)]\n    elif sims[j] >= C_TH:\n        pred = ref_labels[j]\n    else:\n        pred = \"unknown\"\n\n    rows.append((img_path.name, pred))\n\n# â”€â”€ 5ï¸âƒ£ Write submission.csv â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsub = pd.DataFrame(rows, columns=['image_name', 'employee_id'])\nsub.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"âœ… submission.csv saved:\", sub.shape)\ndisplay(sub.head())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}