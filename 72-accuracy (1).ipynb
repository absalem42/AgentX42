{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":105039,"databundleVersionId":12772268,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"b8be7787-ff46-4a01-913c-e157e2823f57","_cell_guid":"8d23f6f9-616a-48c8-a02b-35614c9b210c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Cell 1 :  paths + quick sanity\n# ===============================\nimport os, glob, json, random\nfrom pathlib import Path\nimport pandas as pd\nfrom collections import Counter\nimport torch\n\n# -------- 1. Paths --------\nROOT = Path('/kaggle/input/identity-employees-in-surveillance-cctv/dataset')\nTRAIN_IMG_DIR = ROOT/'train/images'\nTEST_IMG_DIR  = ROOT/'test/images'\nLABELS_CSV    = ROOT/'train/labels.csv'\n\nassert TRAIN_IMG_DIR.exists(), f\"Missing: {TRAIN_IMG_DIR}\"\nassert TEST_IMG_DIR.exists(),  f\"Missing: {TEST_IMG_DIR}\"\nassert LABELS_CSV.exists(),    f\"Missing: {LABELS_CSV}\"\n\n# -------- 2. GPU check (useful later) --------\nprint(\"CUDA available :\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU name       :\", torch.cuda.get_device_name(0))\n\n# -------- 3. Quick stats --------\ntrain_files = glob.glob(str(TRAIN_IMG_DIR/'*.jpg'))\ntest_files  = glob.glob(str(TEST_IMG_DIR/'*.jpg'))\nprint(f\"Train images   : {len(train_files):,}\")\nprint(f\"Test  images   : {len(test_files):,}\")\n\nlabels_df = pd.read_csv(LABELS_CSV)\ndisplay(labels_df.head())\n\nemp_counts = Counter(labels_df['emp_id'])\nprint(f\"Unique employees: {len(emp_counts)}\")\nprint(\"Five smallest classes:\", emp_counts.most_common()[-5:])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:07:43.290332Z","iopub.execute_input":"2025-06-23T17:07:43.290815Z","iopub.status.idle":"2025-06-23T17:07:45.349074Z","shell.execute_reply.started":"2025-06-23T17:07:43.290786Z","shell.execute_reply":"2025-06-23T17:07:45.348320Z"}},"outputs":[{"name":"stdout","text":"CUDA available : True\nGPU name       : Tesla T4\nTrain images   : 1,179\nTest  images   : 636\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"        filename  emp_id\n0  face_0568.jpg  emp016\n1  face_0433.jpg  emp014\n2  face_1751.jpg  emp004\n3  face_0675.jpg  emp028\n4  face_0112.jpg  emp001","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>filename</th>\n      <th>emp_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>face_0568.jpg</td>\n      <td>emp016</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>face_0433.jpg</td>\n      <td>emp014</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>face_1751.jpg</td>\n      <td>emp004</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>face_0675.jpg</td>\n      <td>emp028</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>face_0112.jpg</td>\n      <td>emp001</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"Unique employees: 34\nFive smallest classes: [('emp017', 4), ('emp032', 4), ('emp026', 3), ('emp005', 2), ('emp024', 1)]\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ================================================================\n# Cell A : extract 500 JPG frames / employee from reference video\n#          + save them in EXTRA_DIR  +  create new_rows list\n# ================================================================\nimport cv2, numpy as np\nfrom pathlib import Path\n\nEXTRA_DIR = Path('/kaggle/tmp_ref_frames')\nEXTRA_DIR.mkdir(parents=True, exist_ok=True)\n\nnew_rows = []                 # ‚Üê will be concatenated with labels_df in Cell 2\nSAMPLES  = 500                # number of evenly-spaced frames per video\n\nfor emp_dir in (ROOT / 'reference_faces').iterdir():        # emp001, emp002 ‚Ä¶\n    vid_files = list(emp_dir.glob('*.mp4'))\n    if not vid_files:\n        continue                               # some employees may have only JPGs\n    vid = vid_files[0]\n    cap = cv2.VideoCapture(str(vid))\n    if not cap.isOpened():\n        print(f\"‚ö†Ô∏è could not open {vid}\")\n        continue\n\n    tot = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    # ----- sample SAMPLES indices evenly across the video ----------\n    for f in np.linspace(0, tot - 1, SAMPLES, dtype=int):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, f)\n        ok, frame = cap.read()\n        if not ok:\n            continue\n        fname = f'{emp_dir.name}_{f:04d}.jpg'               # emp001_0003.jpg ‚Ä¶\n        cv2.imwrite(str(EXTRA_DIR / fname), frame)\n        new_rows.append({'filename': fname, 'emp_id': emp_dir.name})\n\n    cap.release()\n\nprint(f\"‚úÖ Saved {len(new_rows):,} extra frames to {EXTRA_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:07:48.595429Z","iopub.execute_input":"2025-06-23T17:07:48.595711Z","iopub.status.idle":"2025-06-23T17:16:06.369599Z","shell.execute_reply.started":"2025-06-23T17:07:48.595689Z","shell.execute_reply":"2025-06-23T17:16:06.368670Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Saved 17,000 extra frames to /kaggle/tmp_ref_frames\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ================================================================\n# Cell 2 : dataset, transforms, stratified split (CCTV hold-out) - new\n# ================================================================\nimport subprocess, sys, importlib.util, numpy as np, torch\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom collections import Counter\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\nimport random\nfrom io import BytesIO\n\n# 0Ô∏è‚É£ Install timm (once)\nif importlib.util.find_spec(\"timm\") is None:\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"-q\", \"install\",\n                    \"timm\", \"torchmetrics\", \"--no-progress\"])\n\n# 1Ô∏è‚É£ CCTV labels + video‚Äêframe extras (Cell A)\nlabels_df = pd.read_csv(LABELS_CSV)      # 1,179 CCTV crops\nextra_df  = pd.DataFrame(new_rows)       # e.g. 300 frames √ó each emp\nlabels_df = pd.concat([labels_df, extra_df], ignore_index=True)\n\n# 1.1Ô∏è‚É£ Add static reference JPGs as ‚Äúphoto‚Äù rows\nstatic_rows = []\nfor emp_dir in (ROOT/'reference_faces').iterdir():\n    for jpg in emp_dir.glob('*.jpg'):\n        static_rows.append({'filename': jpg.name, 'emp_id': emp_dir.name})\n        # Symlink into EXTRA_DIR for gallery lookup\n        if not (EXTRA_DIR/jpg.name).exists():\n            (EXTRA_DIR/jpg.name).symlink_to(jpg)\nlabels_df = pd.concat([labels_df, pd.DataFrame(static_rows)], ignore_index=True)\n\n# 2Ô∏è‚É£ Map employee IDs ‚Üí numeric labels\nemployee_ids = sorted(labels_df.emp_id.unique())\nemp2idx      = {e:i for i,e in enumerate(employee_ids)}\nlabels_df[\"label_idx\"] = labels_df.emp_id.map(emp2idx)\n\n# 3Ô∏è‚É£ Tag each row as CCTV vs ‚Äúphoto‚Äù (frame or static JPG)\nlabels_df[\"source\"] = labels_df.filename.apply(\n    lambda fn: \"photo\" if (EXTRA_DIR/fn).exists() else \"cctv\"\n)\n\n# 4Ô∏è‚É£ Paths to look for images\nIMG_ROOTS = [TRAIN_IMG_DIR, EXTRA_DIR]\n\n# ‚Äî custom JPEG-compression transform (PIL-only) ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî\nclass JpegCompression:\n    def __init__(self, quality=(30,60)):\n        self.qmin, self.qmax = quality\n    def __call__(self, img: Image.Image) -> Image.Image:\n        buf = BytesIO()\n        q   = random.randint(self.qmin, self.qmax)\n        img.save(buf, format=\"JPEG\", quality=q)\n        buf.seek(0)\n        return Image.open(buf)\n\n# 5Ô∏è‚É£ Heavy-augmented train transforms\ntrain_tfms = T.Compose([\n    # PIL-based ‚Äúsurveillance‚Äù degradations\n    T.RandomResizedCrop(224, scale=(0.8,1.0)),\n    T.RandomHorizontalFlip(0.5),\n    T.ColorJitter(0.2,0.2,0.2,0.1),\n    T.RandomRotation(15),\n    T.RandomPerspective(distortion_scale=0.3, p=0.2),\n    T.RandomApply([T.GaussianBlur(3)], p=0.2),\n    T.RandomApply([JpegCompression((30,60))], p=0.3),\n\n    # convert to tensor + normalize\n    T.ToTensor(),\n    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n\n    # occlusion simulation (must go after ToTensor)\n    T.RandomErasing(p=0.3, scale=(0.02,0.10), ratio=(0.3,3.3)),\n])\n\n# 6Ô∏è‚É£ Validation transforms (clean)\nval_tfms = T.Compose([\n    T.Resize(256), T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225]),\n])\n\n# 7Ô∏è‚É£ Dataset class (multi-root lookup)\nclass FaceDS(Dataset):\n    def __init__(self, df, tfm):\n        self.df, self.tfm = df.reset_index(drop=True), tfm\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, i):\n        row = self.df.iloc[i]\n        for root in IMG_ROOTS:\n            path = root/row.filename\n            if path.exists():\n                img = Image.open(path).convert(\"RGB\")\n                break\n        else:\n            raise FileNotFoundError(f\"Missing {row.filename}\")\n        return self.tfm(img), row.label_idx\n\nfull_ds = FaceDS(labels_df, train_tfms)\n\n# 8Ô∏è‚É£ Stratified 10 % hold-out on CCTV only (photos always train)\ncctv_idx  = labels_df.index[labels_df.source==\"cctv\"].to_numpy()\ncctv_lbls = labels_df.label_idx.loc[cctv_idx].to_numpy()\n\n# keep singleton classes entirely in train\nfrom collections import Counter\ncnt = Counter(cctv_lbls)\nsingle_cls = [cls for cls,c in cnt.items() if c==1]\nmask_single = np.isin(cctv_lbls, single_cls)\nsingle_idx  = cctv_idx[mask_single]\nmulti_idx   = cctv_idx[~mask_single]\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.10, random_state=42)\ntrain_multi, val_multi = next(sss.split(\n    multi_idx,\n    labels_df.label_idx.loc[multi_idx]\n))\ntrain_cctv = np.concatenate([single_idx, multi_idx[train_multi]])\nval_cctv   = multi_idx[val_multi]\n\nphoto_idx = labels_df.index[labels_df.source==\"photo\"].to_numpy()\n\ntrain_idx = np.concatenate([train_cctv, photo_idx])\nval_idx   = val_cctv\n\n# 9Ô∏è‚É£ DataLoaders\ntrain_ds = Subset(full_ds, train_idx)\nval_ds   = Subset(FaceDS(labels_df.loc[val_idx], val_tfms),\n                  np.arange(len(val_idx)))\n\nBATCH = 32\ntrain_dl = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n                      num_workers=4, pin_memory=True)\nval_dl   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False,\n                      num_workers=4, pin_memory=True)\n\nprint(f\"Train images: {len(train_ds):,} | Val images: {len(val_ds):,}\")\nprint(\"Smallest training classes:\",\n      Counter(labels_df.label_idx.loc[train_idx]).most_common()[-5:])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:51:38.788386Z","iopub.execute_input":"2025-06-23T17:51:38.789263Z","iopub.status.idle":"2025-06-23T17:51:39.807629Z","shell.execute_reply.started":"2025-06-23T17:51:38.789213Z","shell.execute_reply":"2025-06-23T17:51:39.806788Z"}},"outputs":[{"name":"stdout","text":"Train images: 18,513 | Val images: 118\nSmallest training classes: [(17, 517), (5, 515), (29, 512), (24, 511), (0, 15)]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# # Cell 2.5 : Quick A/B Macro-Accuracy check (3 epochs only)\n# # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# import numpy as np\n# import timm, torch\n# from sklearn.model_selection import StratifiedShuffleSplit\n# from torch.utils.data import Subset, DataLoader\n# from torchmetrics.classification import MulticlassAccuracy\n\n# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# def run_3ep(df, label):\n#     # build new train/val indices just like in Cell 2 ‚Ä¶\n#     cctv_idx  = df.index[df.source == \"cctv\"].to_numpy()\n#     cctv_lbls = df.label_idx.loc[cctv_idx].to_numpy()\n#     from collections import Counter\n#     cnt = Counter(cctv_lbls)\n#     single = {cls for cls,c in cnt.items() if c == 1}\n#     mask_s  = np.isin(cctv_lbls, list(single))\n#     single_i= cctv_idx[mask_s]\n#     multi_i = cctv_idx[~mask_s]\n#     sss     = StratifiedShuffleSplit(1, test_size=0.10, random_state=0)\n#     tr_m, va_m = next(sss.split(multi_i, df.label_idx.loc[multi_i]))\n#     train_i = np.concatenate([single_i, multi_i[tr_m], \n#                               df.index[df.source==\"photo\"].to_numpy()])\n#     val_i   = multi_i[va_m]\n\n#     # build DataLoaders\n#     tr_ds = Subset(FaceDS(df.loc[train_i], train_tfms),\n#                    np.arange(len(train_i)))\n#     vl_ds = Subset(FaceDS(df.loc[val_i],   val_tfms),\n#                    np.arange(len(val_i)))\n#     tr_dl = DataLoader(tr_ds, batch_size=64, shuffle=True,  num_workers=2)\n#     vl_dl = DataLoader(vl_ds, batch_size=64, shuffle=False, num_workers=2)\n\n#     # lightweight 3-epoch run\n#     model = timm.create_model('mobilenetv2_100',\n#                               pretrained=True,\n#                               num_classes=len(employee_ids)).to(device)\n#     opt   = torch.optim.AdamW(model.parameters(), lr=3e-4)\n#     metr  = MulticlassAccuracy(num_classes=len(employee_ids),\n#                                average='macro').to(device)\n\n#     print(f\"\\n>>> {label} | train {len(tr_ds):,} ‚Üí val {len(vl_ds):,}\")\n#     for ep in range(1,4):\n#         model.train()\n#         for x,y in tr_dl:\n#             x,y = x.to(device), y.to(device)\n#             opt.zero_grad()\n#             loss = torch.nn.functional.cross_entropy(model(x), y)\n#             loss.backward(); opt.step()\n#         model.eval(); metr.reset()\n#         with torch.no_grad():\n#             for x,y in vl_dl:\n#                 x,y = x.to(device), y.to(device)\n#                 metr.update(model(x).argmax(1), y)\n#         print(f\" Epoch {ep} | Val Macro {metr.compute():.4f}\")\n\n# # run both variants\n# run_3ep(labels_df[labels_df.source==\"cctv\"], \"CCTV only\")\n# run_3ep(labels_df,                         \"CCTV + video/photo\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T17:51:43.090885Z","iopub.execute_input":"2025-06-23T17:51:43.091178Z","iopub.status.idle":"2025-06-23T17:59:33.143130Z","shell.execute_reply.started":"2025-06-23T17:51:43.091156Z","shell.execute_reply":"2025-06-23T17:59:33.142231Z"}},"outputs":[{"name":"stdout","text":"\n>>> CCTV only | train 1,061 ‚Üí val 118\n Epoch 1 | Val Macro 0.1323\n Epoch 2 | Val Macro 0.2026\n Epoch 3 | Val Macro 0.2439\n\n>>> CCTV + video/photo | train 18,513 ‚Üí val 118\n Epoch 1 | Val Macro 0.2895\n Epoch 2 | Val Macro 0.4789\n Epoch 3 | Val Macro 0.5492\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# # --------------------------------------\n# # Check for employees with no CCTV crop\n# # --------------------------------------\n# import pandas as pd\n\n# # Build a DataFrame of counts by (emp_id, source)\n# counts = pd.crosstab(labels_df.emp_id, labels_df.source)\n# # Employees where cctv==0 & photo>0\n# clean_only = counts[(counts.cctv==0) & (counts.photo>0)]\n# print(\"Employees with only 'photo' examples:\", clean_only.index.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:04:48.807443Z","iopub.execute_input":"2025-06-23T18:04:48.808061Z","iopub.status.idle":"2025-06-23T18:04:48.822006Z","shell.execute_reply.started":"2025-06-23T18:04:48.808037Z","shell.execute_reply":"2025-06-23T18:04:48.821358Z"}},"outputs":[{"name":"stdout","text":"Employees with only 'photo' examples: ['emp029']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# ================================================================\n# Cell 3 : EfficientNet-B0 + ArcFace fine-tune (100 epochs)\n# ================================================================\nimport timm, torch, math, numpy as np\nfrom torch import nn\nfrom torchmetrics.classification import MulticlassAccuracy\n\ndevice       = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_CLASSES  = len(employee_ids)                 # set in Cell 2\nEPOCHS       = 50\nCKPT         = '/kaggle/working/effb0_arcface.pt'\n\n# ‚îÄ‚îÄ 1Ô∏è‚É£  Backbone + 512-D projection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nbackbone = timm.create_model(\n    'efficientnet_b0', pretrained=True, num_classes=0, global_pool='avg'\n).to(device)\nproj = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n\n# ‚îÄ‚îÄ 2Ô∏è‚É£  ArcFace additive-margin head ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nclass ArcFace(nn.Module):\n    def __init__(self, in_f, out_f, s=30.0, m=0.50):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(out_f, in_f))\n        nn.init.xavier_uniform_(self.W)\n        self.s, self.m = s, m\n        self.cos_m, self.sin_m = math.cos(m), math.sin(m)\n        self.th, self.mm = math.cos(math.pi - m), math.sin(math.pi - m) * m\n    def forward(self, x, labels):\n        x_n = nn.functional.normalize(x)\n        w_n = nn.functional.normalize(self.W)\n        cos = nn.functional.linear(x_n, w_n)            # cosine Œ∏\n        sin = torch.sqrt(1.0 - torch.clamp(cos**2, 0, 1))\n        phi = cos * self.cos_m - sin * self.sin_m        # cos(Œ∏+m)\n        phi = torch.where(cos > self.th, phi, cos - self.mm)\n        one_hot = torch.zeros_like(cos, device=x.device)\n        one_hot.scatter_(1, labels.view(-1,1), 1.0)\n        logits = (one_hot * phi + (1.0 - one_hot) * cos) * self.s\n        return logits\n\narc = ArcFace(512, NUM_CLASSES).to(device)\n\n# ‚îÄ‚îÄ 3Ô∏è‚É£  Optimizer, warm-up, cosine LR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nparams = list(backbone.parameters()) + list(proj.parameters()) + list(arc.parameters())\nopt    = torch.optim.AdamW(params, lr=3e-4, weight_decay=1e-4)\nwarm   = torch.optim.lr_scheduler.LinearLR(opt, start_factor=0.1, total_iters=2)\nsched  = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS)\n\ncriterion = nn.CrossEntropyLoss(label_smoothing=0.1)\nmetric    = MulticlassAccuracy(num_classes=NUM_CLASSES, average='macro').to(device)\nbest_acc  = 0.0\n\n# ‚îÄ‚îÄ 4Ô∏è‚É£  Training loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nfor ep in range(1, EPOCHS + 1):\n    # ---- train ----\n    backbone.train(); proj.train(); arc.train()\n    for x, y in train_dl:                       # train_dl from Cell 2\n        x, y = x.to(device), y.to(device)\n        feats  = proj(backbone(x))\n        logits = arc(feats, y)\n        loss   = criterion(logits, y)\n\n        opt.zero_grad(); loss.backward(); opt.step()\n\n    # ---- scheduler ----\n    (warm if ep <= 2 else sched).step()\n\n    # ---- validation ----\n    backbone.eval(); proj.eval(); metric.reset()\n    with torch.no_grad():\n        # pre-normalize weight matrix once per epoch\n        Wn = nn.functional.normalize(arc.W, dim=1)      # [C,512]\n        for x, y in val_dl:                             # val_dl from Cell 2\n            x, y = x.to(device), y.to(device)\n            feats = proj(backbone(x))\n            fnorm = nn.functional.normalize(feats, dim=1)\n            preds = torch.matmul(fnorm, Wn.T).argmax(1) # cosine scores\n            metric.update(preds, y)\n\n    acc = metric.compute().item()\n    print(f\"Epoch {ep:03d} | Val Macro-Acc {acc:.4f}\")\n\n    if acc > best_acc:\n        best_acc = acc\n        torch.save({'backbone': backbone.state_dict(),\n                    'proj'    : proj.state_dict(),\n                    'arc'     : arc.state_dict()}, CKPT)\n        print(\"  üî• new best saved\")\n\nprint(f\"\\n‚ñ∂ Best validation Macro-Accuracy: {best_acc:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T18:40:05.555689Z","iopub.execute_input":"2025-06-23T18:40:05.556042Z","iopub.status.idle":"2025-06-23T20:27:01.100410Z","shell.execute_reply.started":"2025-06-23T18:40:05.556000Z","shell.execute_reply":"2025-06-23T20:27:01.099245Z"}},"outputs":[{"name":"stdout","text":"Epoch 001 | Val Macro-Acc 0.1285\n  üî• new best saved\nEpoch 002 | Val Macro-Acc 0.3623\n  üî• new best saved\nEpoch 003 | Val Macro-Acc 0.4680\n  üî• new best saved\nEpoch 004 | Val Macro-Acc 0.4784\n  üî• new best saved\nEpoch 005 | Val Macro-Acc 0.5522\n  üî• new best saved\nEpoch 006 | Val Macro-Acc 0.5100\nEpoch 007 | Val Macro-Acc 0.5460\nEpoch 008 | Val Macro-Acc 0.5932\n  üî• new best saved\nEpoch 009 | Val Macro-Acc 0.6386\n  üî• new best saved\nEpoch 010 | Val Macro-Acc 0.6559\n  üî• new best saved\nEpoch 011 | Val Macro-Acc 0.7068\n  üî• new best saved\nEpoch 012 | Val Macro-Acc 0.6752\nEpoch 013 | Val Macro-Acc 0.6479\nEpoch 014 | Val Macro-Acc 0.5215\nEpoch 015 | Val Macro-Acc 0.5513\nEpoch 016 | Val Macro-Acc 0.6622\nEpoch 017 | Val Macro-Acc 0.6522\nEpoch 018 | Val Macro-Acc 0.6744\nEpoch 019 | Val Macro-Acc 0.6935\nEpoch 020 | Val Macro-Acc 0.7223\n  üî• new best saved\nEpoch 021 | Val Macro-Acc 0.7021\nEpoch 022 | Val Macro-Acc 0.6859\nEpoch 023 | Val Macro-Acc 0.7516\n  üî• new best saved\nEpoch 024 | Val Macro-Acc 0.6318\nEpoch 025 | Val Macro-Acc 0.6922\nEpoch 026 | Val Macro-Acc 0.6907\nEpoch 027 | Val Macro-Acc 0.7002\nEpoch 028 | Val Macro-Acc 0.7450\nEpoch 029 | Val Macro-Acc 0.7761\n  üî• new best saved\nEpoch 030 | Val Macro-Acc 0.7401\nEpoch 031 | Val Macro-Acc 0.7362\nEpoch 032 | Val Macro-Acc 0.7200\nEpoch 033 | Val Macro-Acc 0.7828\n  üî• new best saved\nEpoch 034 | Val Macro-Acc 0.7393\nEpoch 035 | Val Macro-Acc 0.7514\nEpoch 036 | Val Macro-Acc 0.7342\nEpoch 037 | Val Macro-Acc 0.7448\nEpoch 038 | Val Macro-Acc 0.7327\nEpoch 039 | Val Macro-Acc 0.7773\nEpoch 040 | Val Macro-Acc 0.7431\nEpoch 041 | Val Macro-Acc 0.7658\nEpoch 042 | Val Macro-Acc 0.7729\nEpoch 043 | Val Macro-Acc 0.7475\nEpoch 044 | Val Macro-Acc 0.7620\nEpoch 045 | Val Macro-Acc 0.7919\n  üî• new best saved\nEpoch 046 | Val Macro-Acc 0.7655\nEpoch 047 | Val Macro-Acc 0.7804\nEpoch 048 | Val Macro-Acc 0.7550\nEpoch 049 | Val Macro-Acc 0.7945\n  üî• new best saved\nEpoch 050 | Val Macro-Acc 0.7761\n\n‚ñ∂ Best validation Macro-Accuracy: 0.7945\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# ================================================================\n# Cell 4 : build FIQA-filtered gallery + sweep cosine œÑ\n# ================================================================\n\n# 0Ô∏è‚É£ Install InsightFace & ONNX runtime (only once)\n!{sys.executable} -m pip install -q insightface onnxruntime-gpu\n\nimport sys, os, gc\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport timm\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms as T\nfrom insightface.app import FaceAnalysis\n\n# ‚îÄ‚îÄ device & paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndevice    = 'cuda' if torch.cuda.is_available() else 'cpu'\nROOT      = Path('/kaggle/input/identity-employees-in-surveillance-cctv/dataset')\nEXTRA_DIR = Path('/kaggle/tmp_ref_frames')\n\n# ‚îÄ‚îÄ reload trained backbone + projection ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nCKPT     = '/kaggle/working/effb0_arcface.pt'\nbackbone = timm.create_model('efficientnet_b0', pretrained=False,\n                             num_classes=0, global_pool='avg').to(device)\nproj     = nn.Linear(backbone.num_features, 512, bias=False).to(device)\n\nck = torch.load(CKPT, map_location=device)\nbackbone.load_state_dict(ck['backbone'])\nproj.load_state_dict(    ck['proj'])\nbackbone.eval(); proj.eval()\n\n# ‚îÄ‚îÄ FIQA quality model ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nqa = FaceAnalysis(name='buffalo_s')\nqa.prepare(ctx_id=0)\n\n# ‚îÄ‚îÄ transforms for gallery & val ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nval_tfms = T.Compose([\n    T.Resize(256), T.CenterCrop(224),\n    T.ToTensor(),\n    T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\n# ‚îÄ‚îÄ build raw gallery (static JPEGs + video frames) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nref_paths, ref_labels, ref_embs = [], [], []\n\n# 1) high-quality reference photos\nfor emp_dir in (ROOT/'reference_faces').iterdir():\n    for jpg in emp_dir.glob('*.jpg'):\n        img = Image.open(jpg).convert('RGB')\n        ref_paths.append(jpg)\n        ref_labels.append(emp_dir.name)\n        with torch.no_grad():\n            emb = proj(backbone(val_tfms(img).unsqueeze(0).to(device))).squeeze().cpu()\n        ref_embs.append(emb)\n\n# 2) extracted video frames\nfor jpg in EXTRA_DIR.glob('*.jpg'):\n    img = Image.open(jpg).convert('RGB')\n    ref_paths.append(jpg)\n    ref_labels.append(jpg.name.split('_')[0])\n    with torch.no_grad():\n        emb = proj(backbone(val_tfms(img).unsqueeze(0).to(device))).squeeze().cpu()\n    ref_embs.append(emb)\n\nprint(\"Raw gallery size :\", len(ref_embs))\n\n# ‚îÄ‚îÄ FIQA filtering: drop bottom 30% by det_score ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nscores = []\nfor p in tqdm(ref_paths, desc=\"FIQA scoring\"):\n    pil = Image.open(p).convert('RGB')\n    arr = np.array(pil)                 # H√óW√ó3\n    faces = qa.get(arr)                # returns list of Face objects\n    det_score = faces[0].det_score if faces else 0.0\n    scores.append(det_score)\n\nthreshold = np.percentile(scores, 30)\nkeep_ix   = [i for i,s in enumerate(scores) if s >= threshold]\n\nref_embs   = torch.stack([ref_embs[i]    for i in keep_ix])\nref_labels = [ref_labels[i] for i in keep_ix]\nref_norm   = nn.functional.normalize(ref_embs, dim=1)\nprint(\"Filtered gallery:\", ref_norm.shape)\n\n# ‚îÄ‚îÄ embed validation CCTV set & sweep œÑ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nval_embs, val_true = [], []\nidx2emp = {v:k for k,v in emp2idx.items()}\n\nfor xb, yb in tqdm(val_dl, desc=\"val embeddings\"):\n    xb = xb.to(device)\n    with torch.no_grad():\n        feats = backbone(xb)           # [B, features]\n        embs  = proj(feats).cpu()      # [B, 512]\n    val_embs.append(embs)\n    val_true.extend([idx2emp[int(i)] for i in yb])\n\nval_embs = torch.cat(val_embs, dim=0)  # [N_val, 512]\n\ndef macro_acc(true, pred):\n    classes = sorted(set(true))\n    return np.mean([\n        np.mean([t==p for t,p in zip(true,pred) if t==c])\n        for c in classes\n    ])\n\nbest_tau, best_ma = None, -1\nfor œÑ in np.linspace(0.30, 0.55, 26):\n    preds = []\n    for e in val_embs:\n        sims = ref_norm @ nn.functional.normalize(e, dim=0)\n        j    = sims.argmax().item()\n        preds.append(ref_labels[j] if sims[j] >= œÑ else \"unknown\")\n    m = macro_acc(val_true, preds)\n    print(f\"œÑ={œÑ:.2f} ‚Üí Val Macro-Acc {m:.4f}\")\n    if m > best_ma:\n        best_ma, best_tau = m, œÑ\n\nprint(f\"\\n‚ñ∂ Best œÑ = {best_tau:.2f}  (Val Macro-Acc = {best_ma:.4f})\")\n\n# ‚îÄ‚îÄ save gallery + threshold for Cell 5 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ntorch.save({\n    'ref_norm':   ref_norm,\n    'ref_labels': ref_labels,\n    'best_tau':   best_tau\n}, '/kaggle/working/gallery.pt')\n\n# cleanup\ngc.collect()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T21:09:26.756760Z","iopub.execute_input":"2025-06-23T21:09:26.757792Z","iopub.status.idle":"2025-06-23T21:24:05.662530Z","shell.execute_reply.started":"2025-06-23T21:09:26.757746Z","shell.execute_reply":"2025-06-23T21:24:05.661551Z"}},"outputs":[{"name":"stdout","text":"Applied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_s/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_s/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_s/det_500m.onnx detection [1, 3, '?', '?'] 127.5 128.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_s/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\nApplied providers: ['CUDAExecutionProvider', 'CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}, 'CUDAExecutionProvider': {'sdpa_kernel': '0', 'use_tf32': '1', 'fuse_conv_bias': '0', 'prefer_nhwc': '0', 'tunable_op_max_tuning_duration_ms': '0', 'enable_skip_layer_norm_strict_mode': '0', 'tunable_op_tuning_enable': '0', 'tunable_op_enable': '0', 'use_ep_level_unified_stream': '0', 'device_id': '0', 'has_user_compute_stream': '0', 'gpu_external_empty_cache': '0', 'cudnn_conv_algo_search': 'EXHAUSTIVE', 'cudnn_conv1d_pad_to_nc1d': '0', 'gpu_mem_limit': '18446744073709551615', 'gpu_external_alloc': '0', 'gpu_external_free': '0', 'arena_extend_strategy': 'kNextPowerOfTwo', 'do_copy_in_default_stream': '1', 'enable_cuda_graph': '0', 'user_compute_stream': '0', 'cudnn_conv_use_max_workspace': '1'}}\nfind model: /root/.insightface/models/buffalo_s/w600k_mbf.onnx recognition ['None', 3, 112, 112] 127.5 127.5\nset det-size: (640, 640)\nRaw gallery size : 21867\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"FIQA scoring:   0%|          | 0/21867 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e90653bdd7d340e4926e9c744672b360"}},"metadata":{}},{"name":"stdout","text":"Filtered gallery: torch.Size([21867, 512])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"val embeddings:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa527faaccd34354bdae3df824267a04"}},"metadata":{}},{"name":"stdout","text":"œÑ=0.30 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.31 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.32 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.33 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.34 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.35 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.36 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.37 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.38 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.39 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.40 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.41 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.42 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.43 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.44 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.45 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.46 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.47 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.48 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.49 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.50 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.51 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.52 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.53 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.54 ‚Üí Val Macro-Acc 0.7750\nœÑ=0.55 ‚Üí Val Macro-Acc 0.7750\n\n‚ñ∂ Best œÑ = 0.30  (Val Macro-Acc = 0.7750)\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"100"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"# ================================================================\n# Cell 4B : grid-search (P_TH, œÑ) on validation set\n# ================================================================\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport timm\nfrom itertools import product\n\ndevice      = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_CLASSES = len(employee_ids)\n\n# ‚îÄ‚îÄ 1Ô∏è‚É£ Redefine ArcFace to match your checkpoint keys ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nclass ArcFace(nn.Module):\n    def __init__(self, in_f, out_f, s=30.0, m=0.50):\n        super().__init__()\n        self.W = nn.Parameter(torch.randn(out_f, in_f))\n        nn.init.xavier_uniform_(self.W)\n        self.s, self.m = s, m\n        self.cos_m, self.sin_m = np.cos(m), np.sin(m)\n        self.th, self.mm = np.cos(np.pi - m), np.sin(np.pi - m) * m\n\n    def forward(self, x, y):\n        x_n = nn.functional.normalize(x)\n        w_n = nn.functional.normalize(self.W)\n        cos = nn.functional.linear(x_n, w_n)\n        sin = torch.sqrt(1.0 - torch.clamp(cos**2, 0, 1))\n        phi = torch.where(cos > self.th,\n                          cos * self.cos_m - sin * self.sin_m,\n                          cos - self.mm)\n        one_hot = torch.zeros_like(cos, device=x.device)\n        one_hot.scatter_(1, y.view(-1,1), 1.0)\n        logits = (one_hot * phi + (1.0 - one_hot) * cos) * self.s\n        return logits\n\n# ‚îÄ‚îÄ 2Ô∏è‚É£ Reload your classifier head to get soft-max predictions ‚îÄ‚îÄ‚îÄ\nck = torch.load('/kaggle/working/effb0_arcface.pt', map_location=device)\n\nbackbone = timm.create_model('efficientnet_b0', pretrained=False,\n                             num_classes=0, global_pool='avg').to(device)\nproj     = nn.Linear(backbone.num_features, 512, bias=False).to(device)\narc      = ArcFace(512, NUM_CLASSES).to(device)\n\nbackbone.load_state_dict(ck['backbone'])\nproj.load_state_dict(ck['proj'])\narc.load_state_dict( ck['arc'] )\n\nbackbone.eval(); proj.eval(); arc.eval()\n\n@torch.no_grad()\ndef softmax_vec(x):\n    feats  = proj(backbone(x))\n    dummy  = torch.zeros(len(x), dtype=torch.long, device=device)\n    logits = arc(feats, dummy)\n    return torch.softmax(logits, dim=1)\n\n# ‚îÄ‚îÄ 3Ô∏è‚É£ Precompute soft-max on your val set ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\npvecs = []\nfor imgs, _ in val_dl:\n    pvecs.append( softmax_vec(imgs.to(device)).cpu() )\npvecs_val = torch.cat(pvecs)  # [N_val, NUM_CLASSES]\n\n# ‚îÄ‚îÄ 4Ô∏è‚É£ Macro-accuracy helper ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\ndef macro_acc(true, pred):\n    classes = sorted(set(true))\n    return np.mean([\n        np.mean([t==p for t,p in zip(true,pred) if t==c])\n        for c in classes\n    ])\n\n# (we assume the following variables are in scope from Cell 4):\n#   ref_norm    # torch.Tensor [N_ref, 512]\n#   ref_labels  # list[str] of length N_ref\n#   best_tau    # float from Cell 4\n#   val_embs    # torch.Tensor [N_val, 512]\n#   val_true    # list[str] of length N_val\n\n# ‚îÄ‚îÄ 5Ô∏è‚É£ Grid-search over P_TH ‚àà [0.50,0.70] and œÑ ¬±0.03 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nP_grid = np.arange(0.50, 0.71, 0.02)\nT_grid = np.arange(best_tau-0.03, best_tau+0.031, 0.01)\n\nbest_pair, best_ma = None, -1\nprint(f\"Searching P√óœÑ grid with {len(P_grid)}√ó{len(T_grid)} combos‚Ä¶\")\n\nfor P_TH, œÑ in product(P_grid, T_grid):\n    preds = []\n    for prob_vec, emb in zip(pvecs_val, val_embs):\n        top_p, top_i = prob_vec.max(0)\n        if top_p >= P_TH:\n            preds.append(employee_ids[int(top_i)])\n        else:\n            sims = ref_norm @ nn.functional.normalize(emb, dim=0)\n            j    = int(sims.argmax())\n            preds.append(ref_labels[j] if sims[j] >= œÑ else \"unknown\")\n    m = macro_acc(val_true, preds)\n    if m > best_ma:\n        best_ma, best_pair = m, (P_TH, œÑ)\n    print(f\"P={P_TH:.2f}, œÑ={œÑ:.2f} ‚Üí Val Macro-Acc {m:.4f}\")\n\nprint(f\"\\n‚ñ∂ BEST thresholds: P_TH = {best_pair[0]:.2f}, œÑ = {best_pair[1]:.2f}   (Val Macro-Acc = {best_ma:.4f})\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T21:30:29.247630Z","iopub.execute_input":"2025-06-23T21:30:29.247950Z","iopub.status.idle":"2025-06-23T21:30:31.097542Z","shell.execute_reply.started":"2025-06-23T21:30:29.247920Z","shell.execute_reply":"2025-06-23T21:30:31.096706Z"}},"outputs":[{"name":"stdout","text":"Searching P√óœÑ grid with 11√ó7 combos‚Ä¶\nP=0.50, œÑ=0.27 ‚Üí Val Macro-Acc 0.7988\nP=0.50, œÑ=0.28 ‚Üí Val Macro-Acc 0.7988\nP=0.50, œÑ=0.29 ‚Üí Val Macro-Acc 0.7988\nP=0.50, œÑ=0.30 ‚Üí Val Macro-Acc 0.7988\nP=0.50, œÑ=0.31 ‚Üí Val Macro-Acc 0.7988\nP=0.50, œÑ=0.32 ‚Üí Val Macro-Acc 0.7988\nP=0.50, œÑ=0.33 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.27 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.28 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.29 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.30 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.31 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.32 ‚Üí Val Macro-Acc 0.7988\nP=0.52, œÑ=0.33 ‚Üí Val Macro-Acc 0.7988\nP=0.54, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.54, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.54, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.54, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.54, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.54, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.54, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.56, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.58, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.60, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.62, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.64, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.66, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.68, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.27 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.28 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.29 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.30 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.31 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.32 ‚Üí Val Macro-Acc 0.8037\nP=0.70, œÑ=0.33 ‚Üí Val Macro-Acc 0.8037\n\n‚ñ∂ BEST thresholds: P_TH = 0.54, œÑ = 0.27   (Val Macro-Acc = 0.8037)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# Pk={}\n# for cls_idx, emp in enumerate(employee_ids):\n#     conf=[]\n#     with torch.no_grad():\n#         for imgs, labels in val_dl:\n#             probs = softmax_vec(imgs.to(device)).cpu()\n#             mask  = labels==cls_idx\n#             conf.extend(probs[mask, cls_idx].numpy())\n#     Pk[emp] = np.percentile(conf, 10)   # 10-th percentile\n\n# print(\"Example thresholds:\", list(Pk.items())[:4])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ================================================================\n# Cell 5 : hybrid soft-max + gallery ‚Üí submission.csv\n# ================================================================\nimport torch\nimport pandas as pd\nimport timm\nfrom PIL import Image\n\ndevice      = 'cuda' if torch.cuda.is_available() else 'cpu'\nNUM_CLASSES = len(employee_ids)\nCKPT        = '/kaggle/working/effb0_arcface.pt'\n\n# ‚îÄ‚îÄ 1Ô∏è‚É£ Plug in your best thresholds ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nP_TH = 0.54   # from Cell 4B\nC_TH = 0.27   # from Cell 4B\n\n# ‚îÄ‚îÄ 2Ô∏è‚É£ Reload backbone + ArcFace head ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n# (ArcFace class must already be defined in your notebook)\nbackbone = timm.create_model('efficientnet_b0', pretrained=False,\n                             num_classes=0, global_pool='avg').to(device)\nproj     = torch.nn.Linear(backbone.num_features, 512, bias=False).to(device)\narc      = ArcFace(512, NUM_CLASSES).to(device)\n\nck = torch.load(CKPT, map_location=device)\nbackbone.load_state_dict( ck['backbone'] )\nproj.load_state_dict(     ck['proj']     )\narc.load_state_dict(      ck['arc']      )\n\nbackbone.eval(); proj.eval(); arc.eval()\n\n# ‚îÄ‚îÄ 3Ô∏è‚É£ Define helper functions ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n@torch.no_grad()\ndef softmax_vec(x):\n    feats  = proj(backbone(x))\n    dummy  = torch.zeros(len(x), dtype=torch.long, device=device)\n    logits = arc(feats, dummy)\n    return torch.softmax(logits, dim=1)\n\n@torch.no_grad()\ndef embed(timg):\n    t = val_tfms(timg).unsqueeze(0).to(device)\n    return proj(backbone(t)).squeeze().cpu()\n\n# ‚îÄ‚îÄ 4Ô∏è‚É£ Inference loop ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nrows = []\nfor img_path in sorted(TEST_IMG_DIR.glob('*.jpg')):\n    pil = Image.open(img_path).convert('RGB')\n    # classifier branch with softmax\n    x      = val_tfms(pil).unsqueeze(0).to(device)\n    pvec   = softmax_vec(x)[0].cpu()\n    top_p, top_i = pvec.max(0)\n\n    # gallery branch with cosine similarity\n    emb    = embed(pil)\n    emb_n  = torch.nn.functional.normalize(emb, dim=0)\n    sims   = ref_norm @ emb_n         # ref_norm & ref_labels from Cell 4\n    j      = int(sims.argmax())\n\n    # hybrid decision\n    if top_p >= P_TH:\n        pred = employee_ids[int(top_i)]\n    elif sims[j] >= C_TH:\n        pred = ref_labels[j]\n    else:\n        pred = \"unknown\"\n\n    rows.append((img_path.name, pred))\n\n# ‚îÄ‚îÄ 5Ô∏è‚É£ Write submission.csv ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nsub = pd.DataFrame(rows, columns=['image_name', 'employee_id'])\nsub.to_csv('/kaggle/working/submission.csv', index=False)\nprint(\"‚úÖ submission.csv saved:\", sub.shape)\ndisplay(sub.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-23T21:31:54.521183Z","iopub.execute_input":"2025-06-23T21:31:54.521986Z","iopub.status.idle":"2025-06-23T21:32:11.275984Z","shell.execute_reply.started":"2025-06-23T21:31:54.521952Z","shell.execute_reply":"2025-06-23T21:32:11.275376Z"}},"outputs":[{"name":"stdout","text":"‚úÖ submission.csv saved: (636, 2)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"      image_name employee_id\n0  face_0002.jpg      emp019\n1  face_0005.jpg      emp014\n2  face_0009.jpg      emp016\n3  face_0012.jpg      emp016\n4  face_0014.jpg      emp002","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>employee_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>face_0002.jpg</td>\n      <td>emp019</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>face_0005.jpg</td>\n      <td>emp014</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>face_0009.jpg</td>\n      <td>emp016</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>face_0012.jpg</td>\n      <td>emp016</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>face_0014.jpg</td>\n      <td>emp002</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}